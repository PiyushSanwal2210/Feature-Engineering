{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a860ad6",
   "metadata": {},
   "source": [
    "########## THEORY QUESTIONS ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe26b3b",
   "metadata": {},
   "source": [
    "Question = 1 >>> What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cff5e9",
   "metadata": {},
   "source": [
    "Ans = A parameter is a variable that is used to pass information into a function, method, or procedure. It acts as a placeholder for the value that you provide (called an argument) when you call the function.\n",
    "\n",
    "üß† In Simple Terms:\n",
    "A parameter is like a labeled container.\n",
    "\n",
    "You define it when writing a function.\n",
    "\n",
    "You fill it with a value (an argument) when calling the function.\n",
    "\n",
    "üí° Example in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9051f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(name):  # 'name' is a parameter\n",
    "    print(f\"Hello, {name}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e87123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Piyush!\n"
     ]
    }
   ],
   "source": [
    "greet(\"Piyush\")  # \"Piyush\" is the argument passed to the parameter 'name'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a322e8b",
   "metadata": {},
   "source": [
    "üìå Types of Parameters (in Python):\n",
    "Positional parameters ‚Äì based on position.\n",
    "\n",
    "Default parameters ‚Äì have a default value.\n",
    "\n",
    "Keyword parameters ‚Äì passed using parameter names.\n",
    "\n",
    "Variable-length parameters ‚Äì *args, **kwargs for multiple values.\n",
    "\n",
    "üîÅ Summary:\n",
    "A parameter is part of a function definition.\n",
    "\n",
    "An argument is the actual value you pass during a function call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678d4bc",
   "metadata": {},
   "source": [
    "Question = 2 >>> What is correlation?\n",
    "What does negative correlation mean??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb1a9c",
   "metadata": {},
   "source": [
    "Ans = Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
    "\n",
    "It tells us how closely two variables move together.\n",
    "\n",
    "The correlation coefficient (usually denoted as r) ranges from ‚Äì1 to +1:\n",
    "\n",
    "Correlation (r)\tMeaning\n",
    "+1\tPerfect positive correlation\n",
    "0\tNo correlation\n",
    "‚Äì1\tPerfect negative correlation\n",
    "\n",
    "üîÅ Example:\n",
    "If we look at study time and exam scores:\n",
    "\n",
    "As study time increases, exam scores usually increase ‚Üí positive correlation.\n",
    "\n",
    "üîª What is Negative Correlation?\n",
    "A negative correlation means that as one variable increases, the other decreases.\n",
    "\n",
    "In other words, they move in opposite directions.\n",
    "\n",
    "The correlation coefficient will be less than 0, down to ‚Äì1.\n",
    "\n",
    "üìå Real-Life Examples of Negative Correlation:\n",
    "Variable A\tVariable B\tWhat Happens\n",
    "Temperature\tSales of winter jackets\tAs temperature goes up, jacket sales go down\n",
    "Exercise frequency\tBody fat percentage\tMore exercise ‚Üí Lower fat percentage\n",
    "Speed of a car\tTime to reach destination\tFaster speed ‚Üí Less time needed\n",
    "\n",
    "üßÆ Quick Note:\n",
    "You can calculate correlation in Python using numpy or pandas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e55a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [10, 8, 6, 4, 2]  # decreases as x increases\n",
    "\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "print(correlation)  # Output: -1.0 (perfect negative correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2b9e8",
   "metadata": {},
   "source": [
    "Question = 3 >>> Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b8281",
   "metadata": {},
   "source": [
    "Ans = Machine Learning (ML) is a subset of Artificial Intelligence (AI) that enables computers to learn from data and make decisions or predictions without being explicitly programmed.\n",
    "\n",
    "Instead of writing rules manually, ML systems identify patterns and learn how to act based on examples (data).\n",
    "\n",
    "üß† Simple Definition:\n",
    "Machine Learning is the science of making computers learn from data so they can improve their performance on a task with experience.\n",
    "\n",
    "üì¶ Main Components of Machine Learning:\n",
    "Here are the key components involved in any Machine Learning system:\n",
    "\n",
    "1. Data\n",
    "The foundation of ML.\n",
    "\n",
    "Can be structured (tables), unstructured (images, text), or semi-structured.\n",
    "\n",
    "The quality and quantity of data directly impact model performance.\n",
    "\n",
    "2. Features\n",
    "Individual measurable properties or characteristics of the data.\n",
    "\n",
    "Example: In house pricing, features could be size, location, and number of rooms.\n",
    "\n",
    "3. Model\n",
    "A mathematical representation of the pattern in the data.\n",
    "\n",
    "It is trained using historical data to make predictions or decisions.\n",
    "\n",
    "Types include: linear regression, decision trees, neural networks, etc.\n",
    "\n",
    "4. Algorithm\n",
    "The procedure or method used to train the model.\n",
    "\n",
    "Examples: Gradient Descent, k-NN, SVM, etc.\n",
    "\n",
    "Chooses how the model will learn patterns from the data.\n",
    "\n",
    "5. Training\n",
    "The process where the model learns from data.\n",
    "\n",
    "Involves feeding input data and adjusting model parameters based on the output.\n",
    "\n",
    "6. Testing / Evaluation\n",
    "After training, the model is evaluated on unseen data to check its performance.\n",
    "\n",
    "Common metrics: accuracy, precision, recall, F1-score, RMSE, etc.\n",
    "\n",
    "7. Prediction / Inference\n",
    "Once trained, the model can be used to make predictions or decisions on new data.\n",
    "\n",
    "8. Feedback Loop (Optional)\n",
    "In some systems, predictions are monitored and the model is updated over time with new data.\n",
    "\n",
    "üìä Types of Machine Learning:\n",
    "Type\tDescription\tExample\n",
    "Supervised\tLearns from labeled data\tEmail spam detection\n",
    "Unsupervised\tLearns patterns from unlabeled data\tCustomer segmentation\n",
    "Reinforcement\tLearns by interacting with an environment\tGame-playing AI (like AlphaGo)\n",
    "Semi-supervised\tMix of labeled and unlabeled data\tText classification with few labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71eb1c",
   "metadata": {},
   "source": [
    "Question = 4 >>> How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdfe75",
   "metadata": {},
   "source": [
    "Ans = In Machine Learning, the loss value is a numeric measure of error ‚Äî it tells you how far off your model‚Äôs predictions are from the actual values.\n",
    "\n",
    "üß† Simple Definition:\n",
    "The lower the loss value, the better your model is doing at learning and making predictions.\n",
    "A high loss means your model is making large errors.\n",
    "\n",
    "üîç Why Is Loss Important?\n",
    "It‚Äôs what the model tries to minimize during training.\n",
    "\n",
    "It guides the learning process by telling the algorithm how much to adjust the model's parameters.\n",
    "\n",
    "Helps in model comparison ‚Äî the model with the lower loss is typically better.\n",
    "\n",
    "üìâ Common Loss Functions:\n",
    "Problem Type\tCommon Loss Function\tDescription\n",
    "Regression\tMean Squared Error (MSE)\tPenalizes larger errors more\n",
    "Mean Absolute Error (MAE)\tPenalizes all errors equally\n",
    "Classification\tCross-Entropy Loss\tMeasures difference between actual and predicted probabilities\n",
    "Hinge Loss\tUsed in Support Vector Machines (SVMs)\n",
    "\n",
    "‚úÖ When Is a Model Considered ‚ÄúGood‚Äù?\n",
    "A good model:\n",
    "\n",
    "Has a low loss value on both training and validation datasets.\n",
    "\n",
    "Generalizes well to unseen data (doesn't just memorize training data).\n",
    "\n",
    "üß™ Example:\n",
    "Say you're training a digit classifier:\n",
    "\n",
    "Epoch 1: Loss = 1.2\n",
    "\n",
    "Epoch 5: Loss = 0.8\n",
    "\n",
    "Epoch 10: Loss = 0.3\n",
    "\n",
    "This shows your model is improving, as the loss is decreasing.\n",
    "\n",
    "‚ö†Ô∏è Important Caveats:\n",
    "Low training loss ‚â† good model if the validation/test loss is high ‚Üí this indicates overfitting.\n",
    "\n",
    "A very low loss could also mean the model has memorized the data (especially if data is small).\n",
    "\n",
    "üîÅ Summary:\n",
    "Loss tells you how bad your model is ‚Äî lower is better.\n",
    "\n",
    "It‚Äôs the core signal the model uses to learn and improve.\n",
    "\n",
    "Combine loss with other metrics (like accuracy, precision, recall) for a full evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0713a",
   "metadata": {},
   "source": [
    "Question = 5 >>> What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159d5f4",
   "metadata": {},
   "source": [
    "Ans = In data analysis and machine learning, variables (features) are usually classified as either continuous or categorical, based on the type of data they represent.\n",
    "\n",
    "üî¢ 1. Continuous Variables\n",
    "A continuous variable is a variable that can take on any numeric value within a given range.\n",
    "It has an infinite number of possible values between any two values.\n",
    "\n",
    "‚úÖ Characteristics:\n",
    "Numeric values\n",
    "\n",
    "Can be measured\n",
    "\n",
    "Values can have decimals/fractions\n",
    "\n",
    "Arithmetic operations (mean, std. deviation, etc.) make sense\n",
    "\n",
    "üìå Examples:\n",
    "Height (e.g., 172.5 cm)\n",
    "\n",
    "Temperature (e.g., 36.6¬∞C)\n",
    "\n",
    "Income (e.g., ‚Çπ45,230.75)\n",
    "\n",
    "Weight, speed, distance, etc.\n",
    "\n",
    "üî† 2. Categorical Variables\n",
    "A categorical variable (also called a qualitative variable) takes on limited, fixed values that represent categories or groups.\n",
    "\n",
    "‚úÖ Characteristics:\n",
    "Represent types, names, or labels\n",
    "\n",
    "Can be text or numbers used as labels\n",
    "\n",
    "Cannot do meaningful arithmetic on them\n",
    "\n",
    "üìå Examples:\n",
    "Gender (Male, Female, Other)\n",
    "\n",
    "Blood Type (A, B, AB, O)\n",
    "\n",
    "Marital Status (Single, Married)\n",
    "\n",
    "Shirt size (S, M, L ‚Üí can be ordinal)\n",
    "\n",
    "üß† Two Types of Categorical Variables:\n",
    "Type\tDescription\tExample\n",
    "Nominal\tNo natural order or ranking\tColors: Red, Blue, Green\n",
    "Ordinal\tHave a logical order or ranking\tEducation: High, Medium, Low\n",
    "\n",
    "üß™ Summary Table:\n",
    "Feature Type\tValues\tExamples\tCan do math?\n",
    "Continuous\tInfinite numeric\tHeight, salary, temperature\t‚úÖ Yes\n",
    "Categorical\tFixed categories\tGender, city, color\t‚ùå No\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730acc2",
   "metadata": {},
   "source": [
    "Question = 6 >>> How do we handle categorical variables in Machine Learning? What are the common techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa644ff",
   "metadata": {},
   "source": [
    "Ans = Machine learning models usually require numerical input ‚Äî they can't work directly with text-based categorical data.\n",
    "\n",
    "So, we need to convert categorical variables into numerical format before feeding them into models. This process is called encoding.\n",
    "\n",
    "üß∞ Common Techniques to Handle Categorical Variables:\n",
    "1. Label Encoding\n",
    "Converts each category to a unique integer.\n",
    "\n",
    "üìå Example:\n",
    "Color\tEncoded\n",
    "Red\t0\n",
    "Green\t1\n",
    "Blue\t2\n",
    "\n",
    "‚úÖ Best for:\n",
    "Ordinal data where order matters (e.g., Low < Medium < High)\n",
    "\n",
    "‚ö†Ô∏è Caution:\n",
    "Not suitable for nominal data because it introduces an artificial order.\n",
    "\n",
    "2. One-Hot Encoding\n",
    "Converts each category into a binary column (0 or 1).\n",
    "\n",
    "üìå Example:\n",
    "Color\tRed\tGreen\tBlue\n",
    "Red\t1\t0\t0\n",
    "Blue\t0\t0\t1\n",
    "\n",
    "‚úÖ Best for:\n",
    "Nominal data with no inherent order\n",
    "\n",
    "Widely used in models like linear regression, tree-based models\n",
    "\n",
    "‚ö†Ô∏è Downside:\n",
    "Can result in many columns if the category has many unique values (high cardinality)\n",
    "\n",
    "3. Ordinal Encoding\n",
    "Assigns ordered integers to ordinal categories.\n",
    "\n",
    "üìå Example:\n",
    "Size\tEncoded\n",
    "Small\t1\n",
    "Medium\t2\n",
    "Large\t3\n",
    "\n",
    "‚úÖ Best for:\n",
    "Ordinal features where order is meaningful\n",
    "\n",
    "4. Frequency / Count Encoding\n",
    "Replaces each category with the frequency (count) of that category in the dataset.\n",
    "\n",
    "üìå Example:\n",
    "City\tCount\n",
    "Mumbai\t500\n",
    "Delhi\t300\n",
    "Jaipur\t100\n",
    "\n",
    "‚úÖ Best for:\n",
    "High-cardinality categorical variables\n",
    "\n",
    "Reduces dimensionality\n",
    "\n",
    "5. Target Encoding (Mean Encoding)\n",
    "Replace each category with the mean of the target variable for that category.\n",
    "\n",
    "üìå Example:\n",
    "If predicting salary:\n",
    "\n",
    "Degree\tAvg Salary\n",
    "Bachelors\t‚Çπ50,000\n",
    "Masters\t‚Çπ70,000\n",
    "\n",
    "‚úÖ Best for:\n",
    "Tree-based models\n",
    "\n",
    "When you have enough data to avoid overfitting\n",
    "\n",
    "‚ö†Ô∏è Use with caution:\n",
    "Can lead to data leakage if not properly cross-validated\n",
    "\n",
    "üöÄ Summary Table:\n",
    "Technique\tSuitable For\tPros\tCons\n",
    "Label Encoding\tOrdinal\tSimple\tImposes false order on nominal\n",
    "One-Hot Encoding\tNominal\tPreserves meaning\tIncreases dimensionality\n",
    "Ordinal Encoding\tOrdered categories\tReflects ranking\tNot suitable for nominal\n",
    "Frequency Encoding\tHigh cardinality\tReduces columns\tCan lose category meaning\n",
    "Target Encoding\tSupervised models\tPowerful for prediction\tRisk of leakage, overfitting\n",
    "\n",
    "üí° Tip:\n",
    "Use scikit-learn‚Äôs OneHotEncoder or LabelEncoder, or pandas.get_dummies() for quick one-hot encoding.\n",
    "\n",
    "For deep learning models, use embedding layers for categorical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65cbac",
   "metadata": {},
   "source": [
    "Question = 7 >>> What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92898dd",
   "metadata": {},
   "source": [
    "Ans = In Machine Learning, the dataset is usually divided into two (or more) parts:\n",
    "\n",
    "Training Set\n",
    "\n",
    "Testing Set\n",
    "\n",
    "This split helps you build and evaluate your model properly.\n",
    "\n",
    "üîß 1. Training a Dataset\n",
    "The training set is the portion of the data used to train the model ‚Äî that is, to help the model learn patterns, relationships, and structure in the data.\n",
    "\n",
    "The model uses this data to adjust its internal parameters (like weights in a neural network).\n",
    "\n",
    "The learning process minimizes the loss/error on the training data.\n",
    "\n",
    "üìå Example:\n",
    "If you're building a model to predict house prices, the training set contains:\n",
    "\n",
    "Features: size, number of rooms, location\n",
    "\n",
    "Target: actual house prices\n",
    "\n",
    "The model learns how features relate to prices.\n",
    "\n",
    "‚úÖ 2. Testing a Dataset\n",
    "The testing set is the data that the model has never seen before.\n",
    "\n",
    "It is used to evaluate the performance of the trained model.\n",
    "\n",
    "This tells you how well your model can generalize to new, unseen data.\n",
    "\n",
    "You calculate metrics like accuracy, precision, recall, RMSE, etc., on this set.\n",
    "\n",
    "üß™ Why Do We Need Both?\n",
    "If you test the model on the same data you trained it on, it may look perfect but fail on new data (this is overfitting).\n",
    "\n",
    "Splitting helps you build a model that performs well in the real world, not just on past data.\n",
    "\n",
    "üî¢ Typical Split Ratios:\n",
    "Set\tCommon Ratio\n",
    "Training\t70‚Äì80%\n",
    "Testing\t20‚Äì30%\n",
    "\n",
    "Sometimes, a validation set is also used:\n",
    "\n",
    "Train ‚Üí Learn the model\n",
    "\n",
    "Validate ‚Üí Tune model hyperparameters\n",
    "\n",
    "Test ‚Üí Final evaluation\n",
    "\n",
    "üñºÔ∏è Summary:\n",
    "Aspect\tTraining Set\tTesting Set\n",
    "Purpose\tTo teach the model\tTo evaluate the model\n",
    "Model access\tUsed during model fitting\tNot used during training\n",
    "Size (approx.)\t70‚Äì80% of the data\t20‚Äì30% of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005713b",
   "metadata": {},
   "source": [
    "Question = 8 >>> What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3dc720",
   "metadata": {},
   "source": [
    "Ans = sklearn.preprocessing is a module in Scikit-learn (sklearn) that provides tools for transforming input data before feeding it into a machine learning model.\n",
    "\n",
    "Preprocessing helps to:\n",
    "\n",
    "Make features more comparable\n",
    "\n",
    "Improve model performance\n",
    "\n",
    "Handle categorical data, missing values, scaling, etc.\n",
    "\n",
    "üß∞ Common Functions and Classes in sklearn.preprocessing:\n",
    "1. Scaling & Normalization\n",
    "Tool\tPurpose\tExample\n",
    "StandardScaler\tStandardizes features (mean=0, std=1)\tGood for linear models, SVM\n",
    "MinMaxScaler\tScales features to a range (e.g., 0‚Äì1)\tGood for neural networks\n",
    "RobustScaler\tScales using median & IQR (robust to outliers)\tIdeal for skewed data\n",
    "Normalizer\tScales samples (rows), not features\tFor text or image data\n",
    "\n",
    "2. Encoding Categorical Features\n",
    "Tool\tPurpose\tExample\n",
    "LabelEncoder\tConverts labels to integers\t['dog', 'cat'] ‚Üí [0, 1]\n",
    "OneHotEncoder\tConverts categories to binary columns\t['red', 'blue'] ‚Üí [[1,0],[0,1]]\n",
    "OrdinalEncoder\tConverts categories to ordered integers\t['low', 'medium', 'high']\n",
    "\n",
    "3. Generating Polynomial Features\n",
    "PolynomialFeatures: Creates combinations of features (e.g., x, x¬≤, xy)\n",
    "\n",
    "Used in polynomial regression\n",
    "\n",
    "4. Handling Missing Values\n",
    "sklearn.impute.SimpleImputer (in sklearn.impute module):\n",
    "\n",
    "Not in preprocessing, but closely related\n",
    "\n",
    "Used to fill in missing values with mean, median, most frequent, etc.\n",
    "\n",
    "‚úÖ Example in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c902105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Standardize numeric data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform([[1.0], [2.0], [3.0]])\n",
    "\n",
    "# Encode categorical data\n",
    "encoder = OneHotEncoder()\n",
    "encoded = encoder.fit_transform([['red'], ['blue'], ['red']]).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ebbeb7",
   "metadata": {},
   "source": [
    "Why It‚Äôs Important:\n",
    "Most ML models assume numerical, scaled input.\n",
    "\n",
    "Poorly preprocessed data leads to bad predictions.\n",
    "\n",
    "sklearn.preprocessing makes it easy to clean and transform data properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66928c9e",
   "metadata": {},
   "source": [
    "Question = 9 >>> What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5cf41",
   "metadata": {},
   "source": [
    "Ans = A test set is a portion of your dataset that you set aside to evaluate the performance of a trained machine learning model.\n",
    "It contains data that the model has never seen during training.\n",
    "\n",
    "üéØ Purpose of a Test Set:\n",
    "To check how well your model generalizes to new, unseen data.\n",
    "\n",
    "To get a realistic estimate of model performance.\n",
    "\n",
    "To avoid overfitting by not evaluating on the same data used for training.\n",
    "\n",
    "üìä Typical Workflow:\n",
    "Split your dataset into:\n",
    "\n",
    "Training set ‚Äì used to train the model.\n",
    "\n",
    "Validation set (optional) ‚Äì used to tune hyperparameters.\n",
    "\n",
    "Test set ‚Äì used for final evaluation.\n",
    "\n",
    "Train the model on the training data.\n",
    "\n",
    "Evaluate it on the test set using metrics like:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1 Score\n",
    "\n",
    "RMSE (for regression)\n",
    "\n",
    "üî¢ Example in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc67f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose x = features, y = labels\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now:\n",
    "# - Train your model on X_train, y_train\n",
    "# - Test its accuracy on X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b259820",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "\n",
    "\n",
    "Feature\tTraining Set\tTest Set\n",
    "\n",
    "\n",
    "Used for\tTraining the model\tEvaluating the model\n",
    "\n",
    "Seen by model\t‚úÖ Yes\t‚ùå No\n",
    "\n",
    "Purpose\tLearn patterns\tCheck performance\n",
    "\n",
    "üö´ Don‚Äôt Do This:\n",
    "\n",
    "\n",
    "Never train and test on the same data ‚Äî it gives misleadingly high accuracy and causes overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ca9bb",
   "metadata": {},
   "source": [
    "Question = 10 >>> How do we split data for model fitting (training and testing) in Python?\n",
    "How do you approach a Machine Learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c2649",
   "metadata": {},
   "source": [
    "Ans = We usually use train_test_split() from Scikit-learn to split our data into training and testing sets.\n",
    "\n",
    "‚úçÔ∏è Example Code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e54b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose x = features, y = target/labels\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda21179",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "test_size=0.2: 20% of the data goes into the test set, 80% into training\n",
    "\n",
    "random_state=42: ensures reproducibility\n",
    "\n",
    "stratify=y (optional): ensures class distribution is the same in train/test (important for classification)\n",
    "\n",
    "üß† 2. How Do You Approach a Machine Learning Problem?\n",
    "Here‚Äôs a structured approach commonly used in real-world ML workflows:\n",
    "\n",
    "üîç Step 1: Understand the Problem\n",
    "What is the goal? (e.g., classification, regression)\n",
    "\n",
    "What is the target variable?\n",
    "\n",
    "What does success look like?\n",
    "\n",
    "üìä Step 2: Explore and Prepare the Data\n",
    "Load and inspect the dataset\n",
    "\n",
    "Handle missing values\n",
    "\n",
    "Analyze distributions, correlations, outliers\n",
    "\n",
    "Convert categorical variables (e.g., using one-hot or label encoding)\n",
    "\n",
    "Normalize or scale numerical features if needed\n",
    "\n",
    "üßπ Step 3: Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14a8145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5432d8",
   "metadata": {},
   "source": [
    "Step 4: Select and Train a Model\n",
    "Choose an algorithm depending on your problem:\n",
    "\n",
    "Problem Type\tExample Models\n",
    "Classification\tLogistic Regression, Random Forest\n",
    "Regression\tLinear Regression, XGBoost\n",
    "Clustering\tK-Means, DBSCAN\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "üìà Step 5: Evaluate the Model\n",
    "Use the test set to see how well the model performs:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "Other metrics: confusion matrix, precision, recall, F1-score, RMSE (for regression)\n",
    "\n",
    "üîÅ Step 6: Tune the Model\n",
    "Use cross-validation (GridSearchCV, RandomizedSearchCV)\n",
    "\n",
    "Adjust hyperparameters (e.g., tree depth, learning rate)\n",
    "\n",
    "üöÄ Step 7: Deploy or Interpret\n",
    "Save the model (joblib, pickle)\n",
    "\n",
    "Interpret using tools like SHAP, LIME, or feature importance\n",
    "\n",
    "Integrate into a web app, API, etc.\n",
    "\n",
    "üß≠ Summary Flowchart:\n",
    "text\n",
    "Copy\n",
    "Edit\n",
    "Define Problem ‚Üí Load Data ‚Üí Explore & Clean ‚Üí Encode & Scale ‚Üí\n",
    "Split Data ‚Üí Train Model ‚Üí Evaluate ‚Üí Tune ‚Üí Interpret/Deploy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89eaa00",
   "metadata": {},
   "source": [
    "Question = 11 >>> Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddb7dd",
   "metadata": {},
   "source": [
    "Ans = Exploratory Data Analysis (EDA) is a critical first step in any machine learning project. It involves analyzing and visualizing the dataset to understand its structure, quality, and key patterns before building a model.\n",
    "\n",
    "üß† Purpose of EDA:\n",
    "EDA helps you understand your data so you can make informed decisions during preprocessing and modeling.\n",
    "\n",
    "‚úÖ Reasons to Perform EDA:\n",
    "1. Understand the Data Structure\n",
    "What are the features and target?\n",
    "\n",
    "What types of variables are present? (Numerical vs Categorical)\n",
    "\n",
    "What is the size and shape of the dataset?\n",
    "\n",
    "üìå Why it matters: Models depend on proper feature types and shapes.\n",
    "\n",
    "2. Identify Missing or Invalid Data\n",
    "Detect missing values (NaN, blanks)\n",
    "\n",
    "Spot invalid entries (e.g., negative ages, typos)\n",
    "\n",
    "üìå Why it matters: Most ML models can‚Äôt handle missing or corrupted data.\n",
    "\n",
    "3. Detect Outliers\n",
    "Use boxplots, z-scores, IQR to find unusual values\n",
    "\n",
    "üìå Why it matters: Outliers can distort model predictions and lead to overfitting.\n",
    "\n",
    "4. Understand Distributions\n",
    "Visualize distributions of numerical features (histograms, KDE plots)\n",
    "\n",
    "Check for skewness, normality\n",
    "\n",
    "üìå Why it matters: Some models assume normal distribution; skewed data might need transformation.\n",
    "\n",
    "5. Reveal Relationships Between Variables\n",
    "Use scatter plots, correlation matrices, and pair plots\n",
    "\n",
    "üìå Why it matters: Helps identify important features or multicollinearity (highly correlated predictors)\n",
    "\n",
    "6. Understand the Target Variable\n",
    "Is it balanced? (for classification)\n",
    "\n",
    "Is it continuous or categorical?\n",
    "\n",
    "üìå Why it matters: Imbalanced classes may require techniques like SMOTE or class weighting.\n",
    "\n",
    "7. Guide Feature Engineering\n",
    "Based on your insights, you might:\n",
    "\n",
    "Create new features\n",
    "\n",
    "Transform or combine existing ones\n",
    "\n",
    "Drop irrelevant or redundant columns\n",
    "\n",
    "\n",
    "Final Thought:\n",
    "‚ÄúIf you don‚Äôt understand your data, your model won‚Äôt either.‚Äù\n",
    "\n",
    "Skipping EDA can lead to:\n",
    "\n",
    "Wrong assumptions\n",
    "\n",
    "Poor feature selection\n",
    "\n",
    "Wasted time and bad models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59888998",
   "metadata": {},
   "source": [
    "Question = 12 >>> What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89174ece",
   "metadata": {},
   "source": [
    "Ans = Correlation is a statistical measure that describes the strength and direction of the relationship between two variables.\n",
    "\n",
    "üîÅ In Simple Terms:\n",
    "Correlation tells you how much one variable changes when another variable changes.\n",
    "\n",
    "üî¢ Correlation Coefficient (r):\n",
    "Ranges from ‚Äì1 to +1\n",
    "\n",
    "Value of r\tMeaning\n",
    "+1\tPerfect positive correlation\n",
    "0\tNo correlation\n",
    "‚Äì1\tPerfect negative correlation\n",
    "\n",
    "‚úÖ Examples:\n",
    "Variables\tType of Correlation\n",
    "Study time ‚Üë, exam score ‚Üë\tPositive correlation\n",
    "Temperature ‚Üë, coat sales ‚Üì\tNegative correlation\n",
    "Shoe size vs IQ\tNo correlation\n",
    "\n",
    "üìê Types of Correlation:\n",
    "Positive Correlation:\n",
    "\n",
    "Both variables increase or decrease together.\n",
    "\n",
    "Example: height and weight\n",
    "\n",
    "Negative Correlation:\n",
    "\n",
    "One variable increases while the other decreases.\n",
    "\n",
    "Example: speed and travel time\n",
    "\n",
    "Zero Correlation:\n",
    "\n",
    "No relationship between the variables.\n",
    "\n",
    "Example: number of pets and blood pressure (usually)\n",
    "\n",
    "üßÆ How to Calculate in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54163572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "print(\"Correlation:\", correlation)\n",
    "# Output: 1.0 ‚Üí Perfect positive correlation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb339030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     x    y\n",
      "x  1.0  1.0\n",
      "y  1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "print(df.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b7a04",
   "metadata": {},
   "source": [
    "Why Correlation is Useful in ML:\n",
    "Helps identify related features (for feature selection)\n",
    "\n",
    "Detects multicollinearity (highly correlated inputs)\n",
    "\n",
    "Understands the relationship between input and target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51b271",
   "metadata": {},
   "source": [
    "Question = 13 >>> What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074bc43a",
   "metadata": {},
   "source": [
    "Ans = A negative correlation means that as one variable increases, the other decreases ‚Äî they move in opposite directions.\n",
    "\n",
    "üìâ In Simple Terms:\n",
    "When X goes up, Y tends to go down, and vice versa.\n",
    "\n",
    "üßÆ Correlation Coefficient (r):\n",
    "A negative value of r (between 0 and ‚Äì1) indicates a negative correlation.\n",
    "\n",
    "Correlation Value\tInterpretation\n",
    "‚Äì1.0\tPerfect negative correlation\n",
    "‚Äì0.5\tModerate negative correlation\n",
    "0\tNo correlation\n",
    "\n",
    "‚úÖ Examples of Negative Correlation:\n",
    "Variable A\tVariable B\tWhat Happens\n",
    "Temperature\tSales of heaters\tTemperature ‚Üë ‚Üí Heater sales ‚Üì\n",
    "Exercise frequency\tBody fat percentage\tExercise ‚Üë ‚Üí Body fat ‚Üì\n",
    "Speed of a car\tTime to reach destination\tSpeed ‚Üë ‚Üí Time ‚Üì\n",
    "\n",
    "üìä Visual Clue:\n",
    "In a scatter plot of two negatively correlated variables:\n",
    "\n",
    "The points slope downward from left to right.\n",
    "\n",
    "üß† Why It Matters in Machine Learning:\n",
    "Helps identify inverse relationships between features\n",
    "\n",
    "Useful in feature selection and data interpretation\n",
    "\n",
    "Avoids multicollinearity by removing strongly correlated features (in opposite directions)\n",
    "\n",
    "üß™ Example in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "700e9c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     x    y\n",
      "x  1.0 -1.0\n",
      "y -1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([10, 8, 6, 4, 2])  # y decreases as x increases\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "print(df.corr())\n",
    "\n",
    "# Output: Correlation value ‚âà -1 (perfect negative correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774accaa",
   "metadata": {},
   "source": [
    "Question = 14 >>> How can you find correlation between variables in Python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3134f4d",
   "metadata": {},
   "source": [
    "Ans = Python provides several easy ways to compute the correlation coefficient between variables, especially using NumPy and Pandas.\n",
    "\n",
    "‚úÖ 1. Using Pandas .corr() Method\n",
    "Best for DataFrames with multiple columns.\n",
    "\n",
    "üîç Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aff0d777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             age    income     score\n",
      "age     1.000000  0.953794  0.921356\n",
      "income  0.953794  1.000000  0.861202\n",
      "score   0.921356  0.861202  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'age': [25, 32, 47, 54, 23],\n",
    "    'income': [50000, 60000, 80000, 120000, 45000],\n",
    "    'score': [60, 70, 75, 80, 50]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Correlation matrix\n",
    "print(df.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199edc75",
   "metadata": {},
   "source": [
    "Values close to 1 or ‚Äì1 mean strong correlation.\n",
    "\n",
    "Values close to 0 mean little or no correlation.\n",
    "\n",
    "‚úÖ 2. Using NumPy np.corrcoef()\n",
    "For two single variable arrays (lists or NumPy arrays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01c8865a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [10, 8, 6, 4, 2]  # Decreases as x increases\n",
    "\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "print(\"Correlation:\", correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657118bb",
   "metadata": {},
   "source": [
    " 3. Visualizing Correlation with a Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f30ed424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eef45b",
   "metadata": {},
   "source": [
    "Optional: Choose Type of Correlation\n",
    "By default, .corr() uses Pearson correlation. You can also use:\n",
    "\n",
    "python\n",
    "\n",
    "\n",
    "df.corr(method='kendall')   # or method='spearman'\n",
    "\n",
    "\n",
    "Method\tBest for\n",
    "\n",
    "Pearson\tLinear relationships\n",
    "\n",
    "Spearman\tMonotonic (ordered) data\n",
    "\n",
    "Kendall\tRank-based, small datasets\n",
    "\n",
    "üß† Summary:\n",
    "\n",
    "Method\tBest For\n",
    "\n",
    "df.corr()\tFull correlation matrix\n",
    "\n",
    "np.corrcoef()\tTwo variables (arrays/lists)\n",
    "\n",
    "sns.heatmap()\tVisualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6dbd8",
   "metadata": {},
   "source": [
    "Question = 15 >>> What is causation? Explain difference between correlation and causation with an example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c24f4",
   "metadata": {},
   "source": [
    "Ans = Causation (or causality) means that one variable directly causes a change in another variable.\n",
    "\n",
    "In simple terms:\n",
    "X causes Y ‚Üí if you change X, Y will change as a result.\n",
    "\n",
    "üîÅ Difference Between Correlation and Causation\n",
    "Concept\tDefinition\tKey Question\n",
    "Correlation\tTwo variables move together (positively or negatively)\tDo X and Y change together?\n",
    "Causation\tOne variable directly influences the other\tDoes X cause Y to change?\n",
    "\n",
    "üß† Key Point:\n",
    "Correlation ‚â† Causation\n",
    "\n",
    "Just because two things happen together doesn't mean one causes the other.\n",
    "\n",
    "üìä Example:\n",
    "üß© Correlation Example (No Causation):\n",
    "Data:\n",
    "Ice cream sales and drowning incidents both increase in summer.\n",
    "\n",
    "Observation:\n",
    "There is a positive correlation between ice cream sales and drowning.\n",
    "\n",
    "Reality:\n",
    "Ice cream does NOT cause drowning.\n",
    "The actual cause is hot weather, a third factor affecting both.\n",
    "\n",
    "‚úÖ Causation Example:\n",
    "Data:\n",
    "Increasing the dose of a medicine reduces a patient‚Äôs fever.\n",
    "\n",
    "Conclusion:\n",
    "The medicine has a causal effect on body temperature.\n",
    "\n",
    "üîç Summary:\n",
    "\n",
    "Feature\tCorrelation\tCausation\n",
    "\n",
    "Definition\tX and Y move together\tX causes Y\n",
    "\n",
    "Directionality\tNot implied\tAlways from cause ‚Üí effect\n",
    "\n",
    "Requires proof?\tNo (statistical measure)\tYes (experiments, controlled tests)\n",
    "\n",
    "Example\tHeight ‚Üî Shoe Size\tSmoking ‚Üí Lung Disease\n",
    "\n",
    "‚ö†Ô∏è Why It Matters in Machine Learning:\n",
    "ML models often detect correlations, not causal relationships.\n",
    "\n",
    "Making decisions based on false causality can lead to bad conclusions.\n",
    "\n",
    "Causation is studied in specialized fields like causal inference, using tools like:\n",
    "\n",
    "Randomized controlled trials (RCTs)\n",
    "\n",
    "Causal diagrams (DAGs)\n",
    "\n",
    "Do-calculus (from Judea Pearl's work)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52d810",
   "metadata": {},
   "source": [
    "Question = 16 >>> What is an Optimizer? What are different types of optimizers? Explain each with an example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb5797",
   "metadata": {},
   "source": [
    "Ans = An optimizer is an algorithm used to adjust the model‚Äôs parameters (like weights in neural networks) in order to minimize the loss function during training.\n",
    "\n",
    "In simple terms:\n",
    "An optimizer tells the model how to learn by updating its weights in the right direction to reduce prediction error.\n",
    "\n",
    "üß† Why Optimizers Matter:\n",
    "They guide how fast and how well a model learns.\n",
    "\n",
    "A good optimizer makes training faster, more accurate, and helps avoid local minima.\n",
    "\n",
    "üß∞ Common Types of Optimizers (Especially in Deep Learning)\n",
    "1. Gradient Descent (GD)\n",
    "Basic idea:\n",
    "Move in the direction of the negative gradient of the loss function to find the minimum.\n",
    "\n",
    "üîß Variants:\n",
    "Batch Gradient Descent: Uses the entire dataset for each step.\n",
    "\n",
    "‚úÖ Accurate gradient\n",
    "\n",
    "‚ùå Very slow on large datasets\n",
    "\n",
    "üìå Example:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "w = w - learning_rate * gradient_of_loss\n",
    "2. Stochastic Gradient Descent (SGD)\n",
    "Uses only one sample at a time to update the weights.\n",
    "\n",
    "Introduces randomness ‚Üí can escape local minima.\n",
    "\n",
    "‚úÖ Pros:\n",
    "Fast and memory-efficient\n",
    "\n",
    "‚ùå Cons:\n",
    "Noisy updates, slower convergence\n",
    "\n",
    "üìå Example (in PyTorch or TensorFlow):\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "3. Mini-Batch Gradient Descent\n",
    "A compromise: Uses a small batch of samples (e.g., 32, 64) at a time.\n",
    "\n",
    "Combines speed of SGD with stability of full batch GD.\n",
    "\n",
    "‚úÖ Most commonly used in practice.\n",
    "\n",
    "4. Momentum\n",
    "Adds a fraction of the previous update to the current one.\n",
    "\n",
    "Helps accelerate learning and smooth noisy updates.\n",
    "\n",
    "üìå Formula:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "v = beta * v - learning_rate * gradient\n",
    "w = w + v\n",
    "‚úÖ Think of it like \"rolling downhill\" with momentum.\n",
    "\n",
    "5. RMSProp (Root Mean Square Propagation)\n",
    "Adapts learning rate for each parameter by dividing by a moving average of past squared gradients.\n",
    "\n",
    "‚úÖ Great for non-stationary or noisy data.\n",
    "\n",
    "6. Adam (Adaptive Moment Estimation)\n",
    "‚úÖ Most popular optimizer in deep learning\n",
    "\n",
    "Combines Momentum and RMSProp:\n",
    "\n",
    "Tracks mean (momentum) and uncertainty (RMS)\n",
    "\n",
    "Adapts learning rate for each parameter.\n",
    "\n",
    "üìå Example:\n",
    "\n",
    "python\n",
    "\n",
    "import torch\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "üìä Comparison Table:\n",
    "\n",
    "Optimizer\tKey Idea\tPros\tCons\n",
    "\n",
    "GD\tFull data per update\tStable updates\tVery slow for big data\n",
    "\n",
    "SGD\tOne sample per update\tFast, simple\tNoisy, less accurate\n",
    "\n",
    "Mini-Batch\tSmall batches per update\tBalanced speed & accuracy\tNeeds tuning\n",
    "\n",
    "Momentum\tAdds speed boost from past steps\tFaster convergence\tAdds extra parameter (Œ≤)\n",
    "\n",
    "RMSProp\tAdaptive learning rate\tWorks well on noisy data\tComplex math\n",
    "\n",
    "Adam\tCombines Momentum + RMSProp\tFast, adaptive, robust\tSlightly more memory usage\n",
    "\n",
    "üß† Summary:\n",
    "An optimizer is your model's \"navigator\" ‚Äî guiding it to the lowest error with smart updates.\n",
    "\n",
    "Use Adam for most deep learning tasks.\n",
    "\n",
    "Understand simpler ones (SGD, Momentum) to build strong foundations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5615d",
   "metadata": {},
   "source": [
    "Question = 17 >>> What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b127c",
   "metadata": {},
   "source": [
    "Ans = sklearn.linear_model is a module in Scikit-learn that contains linear models for both regression and classification problems.\n",
    "\n",
    "These models assume that the relationship between the input features and the target variable is linear.\n",
    "\n",
    "üì¶ Common Models in sklearn.linear_model:\n",
    "\n",
    "Model\tType\tPurpose\n",
    "\n",
    "LinearRegression\tRegression\tPredict continuous values\n",
    "\n",
    "Ridge, Lasso, ElasticNet\tRegression\tAdd regularization\n",
    "\n",
    "LogisticRegression\tClassification\tPredict categories (labels)\n",
    "\n",
    "SGDClassifier, SGDRegressor\tBoth\tUses stochastic gradient descent for large datasets\n",
    "\n",
    "‚úÖ 1. Linear Regression\n",
    "\n",
    "Predicts a continuous target variable using a straight-line fit.\n",
    "\n",
    " 2. Logistic Regression\n",
    "\n",
    "Despite the name, it's a classification algorithm ‚Äî not regression.\n",
    "\n",
    "Used for binary (or multiclass) classification.\n",
    "\n",
    "\n",
    "3. Ridge, Lasso, ElasticNet\n",
    "\n",
    "These are regularized versions of linear regression that help:\n",
    "\n",
    "Prevent overfitting\n",
    "\n",
    "Handle multicollinearity\n",
    "\n",
    "4. SGDClassifier / SGDRegressor\n",
    "\n",
    "Used when you have very large datasets. These models use Stochastic Gradient Descent (SGD) to fit the data incrementally.\n",
    "\n",
    "Why Use sklearn.linear_model?\n",
    "Easy to understand and interpret (especially for beginners)\n",
    "\n",
    "Fast to train and evaluate\n",
    "\n",
    "Often performs surprisingly well on simple or linearly separable data\n",
    "\n",
    "üìä Summary Table:\n",
    "\n",
    "Class Name\tUse Case\tNotes\n",
    "\n",
    "LinearRegression\tRegression\tSimple linear regression\n",
    "\n",
    "LogisticRegression\tClassification\tFor binary or multiclass output\n",
    "\n",
    "Ridge, Lasso, ElasticNet\tRegression\tWith regularization\n",
    "\n",
    "SGDClassifier, SGDRegressor\tClassification/Regression\tFor large-scale problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ce077",
   "metadata": {},
   "source": [
    "Question = 18 >>> What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530903a",
   "metadata": {},
   "source": [
    "Ans = The .fit() method is used to train or fit a machine learning model to your training data.\n",
    "\n",
    "In simple terms:\n",
    "model.fit(X, y) tells the model to learn the relationship between input features X and target values y.\n",
    "\n",
    "üß† What Happens During .fit()?\n",
    "The model analyzes the training data\n",
    "\n",
    "Finds patterns, relationships, or trends\n",
    "\n",
    "Learns internal parameters (like weights in linear regression or neural networks)\n",
    "\n",
    "Prepares itself to make predictions on new data\n",
    "\n",
    "üßæ Required Arguments:\n",
    "\n",
    "model.fit(X, y)\n",
    "Argument\tType\tDescription\n",
    "X\t2D array-like (rows = samples, columns = features)\tInput data (features)\n",
    "y\t1D array-like (labels)\tTarget values (what you want to predict)\n",
    "\n",
    "‚úÖ Example: Linear Regression\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = [[1], [2], [3], [4]]   # Features (e.g., house size)\n",
    "y = [100, 200, 300, 400]   # Target (e.g., price)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "üéØ After .fit() You Can Use:\n",
    "model.predict(X_test) ‚Üí to make predictions\n",
    "\n",
    "model.score(X, y) ‚Üí to evaluate performance\n",
    "\n",
    "model.coef_, model.intercept_ ‚Üí for coefficients (in linear models)\n",
    "\n",
    "‚ö†Ô∏è Notes:\n",
    "Make sure X and y are the same length (same number of samples)\n",
    "\n",
    "X must be 2D even if there's only 1 feature (reshape if needed)\n",
    "\n",
    "python\n",
    "\n",
    "X = np.array([1, 2, 3]).reshape(-1, 1)\n",
    "üß™ Summary:\n",
    "\n",
    "Function\tPurpose\n",
    "\n",
    "fit(X, y)\tTrain the model on your data\n",
    "\n",
    "predict(X)\tPredict for new input\n",
    "\n",
    "score(X, y)\tEvaluate model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2568a",
   "metadata": {},
   "source": [
    "Question = 19 >>> What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c191887",
   "metadata": {},
   "source": [
    "Ans = The .predict() method is used to make predictions using a trained model.\n",
    "\n",
    "In simple terms:\n",
    "model.predict(X) takes input data X and returns the model‚Äôs predicted output (labels or values), based on what it learned during .fit().\n",
    "\n",
    "üß† When Do You Use It?\n",
    "After you‚Äôve trained the model using:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "model.fit(X_train, y_train)\n",
    "Then you can predict:\n",
    "\n",
    "python\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "üßæ Required Argument:\n",
    "Argument\tType\tDescription\n",
    "X\t2D array-like (rows = samples, columns = features)\tInput data to predict for\n",
    "\n",
    "No y is needed ‚Äî predict() does not use true labels.\n",
    "\n",
    "X must have the same shape (i.e., same number of features) as the training data.\n",
    "\n",
    "‚úÖ Example:\n",
    "python\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Training data\n",
    "X = [[1], [2], [3]]\n",
    "y = [100, 200, 300]\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict for new input\n",
    "new_data = [[4], [5]]\n",
    "predictions = model.predict(new_data)\n",
    "\n",
    "print(predictions)\n",
    "# Output: [400. 500.]\n",
    "üß™ Output Type:\n",
    "Model Type\tpredict() Returns\n",
    "Regression\tContinuous values (floats)\n",
    "Classification\tPredicted class labels\n",
    "\n",
    "‚ö†Ô∏è Tips:\n",
    "Make sure the input to predict() has the same number of features as the training data.\n",
    "\n",
    "For classification models, if you want class probabilities instead of labels, use:\n",
    "\n",
    "python\n",
    "\n",
    "model.predict_proba(X)\n",
    "üìå Summary:\n",
    "\n",
    "Method\tPurpose\n",
    "\n",
    "fit(X, y)\tTrain the model\n",
    "\n",
    "predict(X)\tPredict for new/unseen data\n",
    "\n",
    "predict_proba(X)\tReturn class probabilities (classification only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb9349",
   "metadata": {},
   "source": [
    "Question = 20 >>> What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347f36f",
   "metadata": {},
   "source": [
    "Ans = In data science and machine learning, variables (features) are classified based on the type of data they represent. Two main types are:\n",
    "\n",
    "1. üßÆ Continuous Variables\n",
    "These are numeric variables that can take any value within a range.\n",
    "\n",
    "‚úÖ Characteristics:\n",
    "Infinite possibilities (can include decimals)\n",
    "\n",
    "Measured, not counted\n",
    "\n",
    "Often used in regression problems\n",
    "\n",
    "üîç Examples:\n",
    "Height (e.g., 172.5 cm)\n",
    "\n",
    "Temperature (e.g., 36.6¬∞C)\n",
    "\n",
    "Income (e.g., ‚Çπ52,345.75)\n",
    "\n",
    "Time (e.g., 2.5 hours)\n",
    "\n",
    "2. üè∑Ô∏è Categorical Variables\n",
    "These are variables that represent categories or groups. They have a limited number of distinct values.\n",
    "\n",
    "‚úÖ Characteristics:\n",
    "Non-numeric (or treated as such)\n",
    "\n",
    "Can be labels or names\n",
    "\n",
    "Often used in classification problems\n",
    "\n",
    "üìÇ Types of Categorical Variables:\n",
    "a. Nominal (No order)\n",
    "Examples: Gender (Male, Female), Color (Red, Blue, Green)\n",
    "\n",
    "b. Ordinal (Ordered)\n",
    "Examples: Education Level (High School < Bachelor's < Master's), Rating (Poor < Fair < Good < Excellent)\n",
    "\n",
    "üß† Summary Table:\n",
    "\n",
    "Type\tValues\tExamples\tUsed In\n",
    "\n",
    "Continuous\tAny number (real/decimal)\tAge, Salary, Temperature\tRegression Models\n",
    "\n",
    "Categorical\tLimited groups/categories\tGender, Color, Size\tClassification\n",
    "\n",
    "‚öôÔ∏è Handling in ML:\n",
    "\n",
    "Continuous variables ‚Üí Can be used as-is (maybe scaled).\n",
    "\n",
    "Categorical variables ‚Üí Need encoding (e.g., One-Hot, Label Encoding).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114180f",
   "metadata": {},
   "source": [
    "Question = 21 >>> What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69c611",
   "metadata": {},
   "source": [
    "Ans = Feature scaling is a preprocessing technique used to normalize or standardize the range of features (variables) in your dataset.\n",
    "\n",
    "In simple terms:\n",
    "It ensures that all features contribute equally to the model, especially those based on distance or gradient calculations.\n",
    "\n",
    "ü§î Why Is Feature Scaling Important in Machine Learning?\n",
    "Many machine learning algorithms:\n",
    "\n",
    "Are sensitive to the magnitude of input values\n",
    "\n",
    "Assume features are on the same scale\n",
    "\n",
    "Perform poorly when features vary widely in range\n",
    "\n",
    "‚ö†Ô∏è Problem Without Scaling:\n",
    "Feature\tRange\n",
    "Age\t18‚Äì70\n",
    "Income\t20,000‚Äì2,00,000\n",
    "Height\t140‚Äì200 cm\n",
    "\n",
    "Without scaling, features like income dominate calculations due to their large values.\n",
    "\n",
    "üß† Helps In:\n",
    "Algorithm\tNeeds Scaling?\tWhy?\n",
    "KNN, K-Means, SVM\t‚úÖ Yes\tBased on distance metrics\n",
    "Gradient Descent-based (e.g., Logistic Regression, Neural Networks)\t‚úÖ Yes\tConverges faster\n",
    "Tree-based models (e.g., Random Forest, XGBoost)\t‚ùå No\tNot sensitive to feature scale\n",
    "\n",
    "‚öôÔ∏è Common Feature Scaling Methods\n",
    "\n",
    "1. Standardization (Z-score normalization)\n",
    "\n",
    "Transforms data to have mean = 0 and standard deviation = 1v\n",
    "\n",
    "2. Min-Max Scaling\n",
    "\n",
    "Scales values to a fixed range (usually [0, 1])\n",
    "\n",
    "3. Robust Scaling\n",
    "\n",
    "Uses median and IQR; robust to outliers\n",
    "\n",
    " Example:\n",
    "\n",
    "Feature\tRaw Value\tStandardized\tMin-Max Scaled\n",
    "\n",
    "Age\t40\t0.25\t0.44\n",
    "\n",
    "Income\t80,000\t‚Äì0.45\t0.30\n",
    "\n",
    "‚úÖ Benefits of Feature Scaling:\n",
    "\n",
    "Faster convergence in gradient-based algorithms\n",
    "\n",
    "Improved accuracy in distance-based models\n",
    "\n",
    "Fair contribution from all features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63eaa1c",
   "metadata": {},
   "source": [
    "Question = 22 >>> How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a52487",
   "metadata": {},
   "source": [
    "Ans = Scikit-learn (sklearn) provides easy-to-use tools for scaling data with just a few lines of code.\n",
    "\n",
    "‚úÖ Step-by-Step: Scaling with sklearn.preprocessing\n",
    "\n",
    "üì¶ 1. Import the Scaler\n",
    "\n",
    "Choose the scaler based on your data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea599456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868e0be",
   "metadata": {},
   "source": [
    "2. Prepare Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00edc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample feature matrix (rows = samples, columns = features)\n",
    "X = np.array([[1, 100],\n",
    "              [2, 200],\n",
    "              [3, 300],\n",
    "              [4, 400]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3dc83b",
   "metadata": {},
   "source": [
    "3. Apply the Scaler\n",
    "\n",
    "A. Standardization (Z-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0129641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a3c60",
   "metadata": {},
   "source": [
    "B. Min-Max Scaling (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3745bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d5ebf",
   "metadata": {},
   "source": [
    "C. Robust Scaling (based on median & IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00fae54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa894e",
   "metadata": {},
   "source": [
    " 4. Using with Pandas\n",
    " \n",
    "If you're working with a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12d62d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age    Income\n",
      "0 -1.341641 -1.171700\n",
      "1 -0.447214 -0.650945\n",
      "2  0.447214  0.390567\n",
      "3  1.341641  1.432078\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': [25, 30, 35, 40],\n",
    "    'Income': [50000, 60000, 80000, 100000]\n",
    "})\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(df)\n",
    "\n",
    "df_scaled = pd.DataFrame(scaled_values, columns=df.columns)\n",
    "print(df_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39111d8",
   "metadata": {},
   "source": [
    "Tips:\n",
    "\n",
    "Always fit the scaler on training data only:\n",
    "\n",
    "Summary of Scalers:\n",
    "\n",
    "Scaler\tBest For\tHandles Outliers?\n",
    "\n",
    "StandardScaler\tNormally distributed data\t‚ùå No\n",
    "\n",
    "MinMaxScaler\tWhen you need values [0, 1]\t‚ùå No\n",
    "\n",
    "RobustScaler\tData with many outliers\t‚úÖ Yes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afe09a",
   "metadata": {},
   "source": [
    "Question = 23 >>> What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e4fdc",
   "metadata": {},
   "source": [
    "Ans = sklearn.preprocessing is a module in Scikit-learn that provides tools to prepare your data before feeding it into a machine learning model.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "It helps you clean, scale, encode, and transform data so your model can learn from it more effectively.\n",
    "\n",
    "üéØ Why Use sklearn.preprocessing?\n",
    "\n",
    "Most machine learning algorithms expect numerical, well-scaled, and clean input.\n",
    "\n",
    "This module handles common preprocessing tasks like:\n",
    "\n",
    "Scaling features to the same range\n",
    "\n",
    "Encoding categorical variables\n",
    "\n",
    "Handling missing values\n",
    "\n",
    "Normalizing data\n",
    "\n",
    "Generating polynomial features\n",
    "\n",
    "‚öôÔ∏è Common Classes & Functions in sklearn.preprocessing\n",
    "\n",
    "Class / Function\tPurpose\tExample Use\n",
    "\n",
    "StandardScaler\tStandardize features (mean = 0, std = 1)\tUsed in Logistic Regression\n",
    "\n",
    "MinMaxScaler\tScale features to [0, 1]\tUsed in Neural Networks\n",
    "\n",
    "RobustScaler\tScale using median & IQR (for outliers)\tUsed with skewed data\n",
    "\n",
    "LabelEncoder\tConvert labels to numbers\tFor target variable (y) encoding\n",
    "\n",
    "OneHotEncoder\tConvert categories to binary columns\tFor input feature encoding\n",
    "\n",
    "OrdinalEncoder\tConvert ordered categories to integers\tFor ordinal categorical features\n",
    "\n",
    "PolynomialFeatures\tCreate polynomial feature combinations\tFor polynomial regression\n",
    "\n",
    "Normalizer\tNormalize each sample (row) to unit norm\tUsed in text or signal data\n",
    "\n",
    "Binarizer\tConvert numeric data to binary values\tFor threshold-based features\n",
    "\n",
    "‚úÖ Example: Scaling and Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b0dc6",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'Age': [25, 30, 45],\n",
    "    'Gender': ['Male', 'Female', 'Male']\n",
    "})\n",
    "\n",
    "# 1. Scale numerical feature\n",
    "scaler = StandardScaler()\n",
    "df['Age_scaled'] = scaler.fit_transform(df[['Age']])\n",
    "\n",
    "# 2. Encode categorical feature\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "gender_encoded = encoder.fit_transform(df[['Gender']])\n",
    "gender_df = pd.DataFrame(gender_encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# Combine\n",
    "df_final = pd.concat([df, gender_df], axis=1)\n",
    "print(df_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ebb4c",
   "metadata": {},
   "source": [
    "When to Use sklearn.preprocessing?\n",
    "üßº Before model training (as part of pipeline)\n",
    "\n",
    "üß™ After loading and cleaning your raw dataset\n",
    "\n",
    "üîÅ During cross-validation (fit only on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a96bec",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9132c",
   "metadata": {},
   "source": [
    "Question = 24 >>> How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75ba38",
   "metadata": {},
   "source": [
    "Ans = In machine learning, it's essential to split your dataset into:\n",
    "\n",
    "Training set ‚Äì used to train the model\n",
    "\n",
    "Testing set ‚Äì used to evaluate how well the model performs on unseen data\n",
    "\n",
    "This prevents overfitting and ensures your model can generalize.\n",
    "\n",
    "‚úÖ Use train_test_split() from Scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70d06c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24e4d841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 4\n",
      "Testing size: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    'Age': [25, 30, 35, 40, 45, 50],\n",
    "    'Income': [40000, 50000, 60000, 70000, 80000, 90000],\n",
    "    'Purchased': [0, 1, 0, 1, 0, 1]\n",
    "})\n",
    "\n",
    "# Features and Target\n",
    "X = data[['Age', 'Income']]     # Input features\n",
    "y = data['Purchased']           # Target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training size:\", len(X_train))\n",
    "print(\"Testing size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6da9d2",
   "metadata": {},
   "source": [
    "Key Parameters:\n",
    "\n",
    "Parameter\tDescription\n",
    "\n",
    "test_size=0.2\t20% of data goes to the test set\n",
    "\n",
    "random_state=42\tEnsures reproducibility (same split each time)\n",
    "\n",
    "shuffle=True\t(Default) Shuffles data before splitting\n",
    "\n",
    "stratify=y\tMaintains class balance (especially for classification problems)\n",
    "\n",
    "üéØ Tip: Use stratify=y for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d7baead",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de055854",
   "metadata": {},
   "source": [
    "This ensures both sets have the same class distribution.\n",
    "\n",
    "üß† Why Split the Data?\n",
    "\n",
    "Set\tUsed For\n",
    "\n",
    "Training\tFitting the model (learning patterns)\n",
    "\n",
    "Testing\tEvaluating model performance\n",
    "\n",
    "Without splitting, the model may memorize the data instead of learning patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14aeb2",
   "metadata": {},
   "source": [
    "Question = 25 >>> Explain data encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e105ed3",
   "metadata": {},
   "source": [
    "Ans = Data encoding is the process of converting categorical (non-numeric) data into numerical form, so that it can be used by machine learning algorithms ‚Äî most of which require numeric input.\n",
    "\n",
    "ü§î Why is Encoding Needed?\n",
    "Many datasets have categorical features, like:\n",
    "\n",
    "\"Gender\": Male, Female\n",
    "\n",
    "\"Color\": Red, Blue, Green\n",
    "\n",
    "\"Education\": High School, Bachelor's, Master's\n",
    "\n",
    "Machine learning models can‚Äôt directly work with strings or labels. So we encode them into numbers.\n",
    "\n",
    "üîß Common Data Encoding Techniques:\n",
    "‚úÖ 1. Label Encoding\n",
    "Converts categories to integers.\n",
    "\n",
    "Each unique value gets a unique number.\n",
    "\n",
    "üîç Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f2b8051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data = ['red', 'green', 'blue', 'green']\n",
    "encoded = le.fit_transform(data)\n",
    "print(encoded)  # [2 1 0 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a0836a",
   "metadata": {},
   "source": [
    "2. One-Hot Encoding\n",
    "\n",
    "Converts each category into a binary (0 or 1) column.\n",
    "\n",
    "Avoids implying order.\n",
    "\n",
    "üîç Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef34f67",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([['red'], ['green'], ['blue']])\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded = encoder.fit_transform(data)\n",
    "print(encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87c231",
   "metadata": {},
   "source": [
    "Color\tBlue\tGreen\tRed\n",
    "\n",
    "Red\t0\t0\t1\n",
    "\n",
    "Green\t0\t1\t0\n",
    "\n",
    "Blue\t1\t0\t0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf4b41",
   "metadata": {},
   "source": [
    "3. Ordinal Encoding\n",
    "\n",
    "Used for ordered categories (ordinal data)\n",
    "\n",
    "Preserves ranking or hierarchy\n",
    "\n",
    "üîç Example:\n",
    "python\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "data = [['low'], ['medium'], ['high']]\n",
    "\n",
    "encoder = OrdinalEncoder(categories=[['low', 'medium', 'high']])\n",
    "\n",
    "encoded = encoder.fit_transform(data)\n",
    "\n",
    "print(encoded)  # [[0], [1], [2]]\n",
    "\n",
    "Level\tEncoded\n",
    "\n",
    "Low\t0\n",
    "\n",
    "Medium\t1\n",
    "\n",
    "High\t2\n",
    "\n",
    "‚úÖ 4. Binary Encoding / Target Encoding\n",
    "\n",
    "(More advanced, used when there are many categories)\n",
    "\n",
    "Converts categories to binary codes or uses mean of the target (for target encoding)\n",
    "\n",
    "Libraries like category_encoders in Python can help\n",
    "\n",
    "üß† Summary Table:\n",
    "\n",
    "Encoding Type\tUse Case\tKeeps Order?\tRisk of Misuse\n",
    "\n",
    "Label Encoding\tOrdinal or binary categories\t‚úÖ Yes\tImplies order falsely\n",
    "\n",
    "One-Hot Encoding\tNominal categories\t‚ùå No\tHigh dimensionality\n",
    "\n",
    "Ordinal Encoding\tOrdered categories\t‚úÖ Yes\tGood for ordinal only\n",
    "\n",
    "üìå When to Use Which?\n",
    "\n",
    "Scenario\tRecommended Encoding\n",
    "\n",
    "Nominal categories (e.g., color)\tOne-Hot Encoding\n",
    "\n",
    "Ordinal categories (e.g., rank)\tOrdinal Encoding\n",
    "\n",
    "Large cardinality (many categories)\tTarget or Binary Encoding\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
